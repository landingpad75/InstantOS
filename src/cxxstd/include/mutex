#pragma once

#include <atomic>
#include <type_traits>

namespace std {

class spinlock {
    atomic<bool> flag{false};
    
public:
    spinlock() = default;
    spinlock(const spinlock&) = delete;
    spinlock& operator=(const spinlock&) = delete;
    
    void lock() noexcept {
        while (flag.exchange(true, memory_order_acquire)) {
            while (flag.load(memory_order_relaxed)) {
                #if defined(__x86_64__) || defined(__i386__)
                __builtin_ia32_pause();
                #elif defined(__aarch64__) || defined(__arm__)
                __asm__ __volatile__("yield" ::: "memory");
                #endif
            }
        }
    }
    
    bool try_lock() noexcept {
        return !flag.exchange(true, memory_order_acquire);
    }
    
    void unlock() noexcept {
        flag.store(false, memory_order_release);
    }
};

class mutex {
    spinlock lck;
    
public:
    mutex() = default;
    mutex(const mutex&) = delete;
    mutex& operator=(const mutex&) = delete;
    
    void lock() noexcept {
        lck.lock();
    }
    
    bool try_lock() noexcept {
        return lck.try_lock();
    }
    
    void unlock() noexcept {
        lck.unlock();
    }
};

class recursive_mutex {
    spinlock lck;
    atomic<unsigned long> owner{0};
    atomic<unsigned int> count{0};
    
    static unsigned long get_thread_id() noexcept {
        static atomic<unsigned long> next_id{1};
        thread_local unsigned long tid = next_id.fetch_add(1, memory_order_relaxed);
        return tid;
    }
    
public:
    recursive_mutex() = default;
    recursive_mutex(const recursive_mutex&) = delete;
    recursive_mutex& operator=(const recursive_mutex&) = delete;
    
    void lock() noexcept {
        unsigned long tid = get_thread_id();
        
        if (owner.load(memory_order_relaxed) == tid) {
            count.fetch_add(1, memory_order_relaxed);
            return;
        }
        
        lck.lock();
        owner.store(tid, memory_order_relaxed);
        count.store(1, memory_order_relaxed);
    }
    
    bool try_lock() noexcept {
        unsigned long tid = get_thread_id();
        
        if (owner.load(memory_order_relaxed) == tid) {
            count.fetch_add(1, memory_order_relaxed);
            return true;
        }
        
        if (lck.try_lock()) {
            owner.store(tid, memory_order_relaxed);
            count.store(1, memory_order_relaxed);
            return true;
        }
        return false;
    }
    
    void unlock() noexcept {
        if (count.fetch_sub(1, memory_order_relaxed) == 1) {
            owner.store(0, memory_order_relaxed);
            lck.unlock();
        }
    }
};

class timed_mutex {
    mutex mtx;
    
public:
    timed_mutex() = default;
    timed_mutex(const timed_mutex&) = delete;
    timed_mutex& operator=(const timed_mutex&) = delete;
    
    void lock() noexcept {
        mtx.lock();
    }
    
    bool try_lock() noexcept {
        return mtx.try_lock();
    }
    
    void unlock() noexcept {
        mtx.unlock();
    }
    
// chrono required for these:
//     template<typename Rep, typename Period>
//     bool try_lock_for(const chrono::duration<Rep, Period>&) noexcept {
//         return try_lock();
//     }
//     
//     template<typename Clock, typename Duration>
//     bool try_lock_until(const chrono::time_point<Clock, Duration>&) noexcept {
//         return try_lock();
//     }
};

class recursive_timed_mutex {
    recursive_mutex mtx;
    
public:
    recursive_timed_mutex() = default;
    recursive_timed_mutex(const recursive_timed_mutex&) = delete;
    recursive_timed_mutex& operator=(const recursive_timed_mutex&) = delete;
    
    void lock() noexcept {
        mtx.lock();
    }
    
    bool try_lock() noexcept {
        return mtx.try_lock();
    }
    
    void unlock() noexcept {
        mtx.unlock();
    }
    
//    template<typename Rep, typename Period>
//    bool try_lock_for(const chrono::duration<Rep, Period>&) noexcept {
//        return try_lock();
//    }
//    
//    template<typename Clock, typename Duration>
//    bool try_lock_until(const chrono::time_point<Clock, Duration>&) noexcept {
//        return try_lock();
//    }
};

struct defer_lock_t { explicit defer_lock_t() = default; };
struct try_to_lock_t { explicit try_to_lock_t() = default; };
struct adopt_lock_t { explicit adopt_lock_t() = default; };

inline constexpr defer_lock_t defer_lock{};
inline constexpr try_to_lock_t try_to_lock{};
inline constexpr adopt_lock_t adopt_lock{};

template<typename Mutex>
class lock_guard {
    Mutex& m;
    
public:
    using mutex_type = Mutex;
    
    explicit lock_guard(Mutex& mtx) : m(mtx) {
        m.lock();
    }
    
    lock_guard(Mutex& mtx, adopt_lock_t) noexcept : m(mtx) {}
    
    ~lock_guard() {
        m.unlock();
    }
    
    lock_guard(const lock_guard&) = delete;
    lock_guard& operator=(const lock_guard&) = delete;
};

template<typename... Mutexes>
class scoped_lock;

template<>
class scoped_lock<> {
public:
    explicit scoped_lock() = default;
    ~scoped_lock() = default;
    scoped_lock(const scoped_lock&) = delete;
    scoped_lock& operator=(const scoped_lock&) = delete;
};

template<typename Mutex>
class scoped_lock<Mutex> {
    Mutex& m;
    
public:
    using mutex_type = Mutex;
    
    explicit scoped_lock(Mutex& mtx) : m(mtx) {
        m.lock();
    }
    
    scoped_lock(Mutex& mtx, adopt_lock_t) noexcept : m(mtx) {}
    
    ~scoped_lock() {
        m.unlock();
    }
    
    scoped_lock(const scoped_lock&) = delete;
    scoped_lock& operator=(const scoped_lock&) = delete;
};

template<typename Mutex>
class unique_lock {
    Mutex* m;
    bool owns;
    
public:
    using mutex_type = Mutex;
    
    unique_lock() noexcept : m(nullptr), owns(false) {}
    
    explicit unique_lock(Mutex& mtx) : m(&mtx), owns(true) {
        m->lock();
    }
    
    unique_lock(Mutex& mtx, defer_lock_t) noexcept : m(&mtx), owns(false) {}
    
    unique_lock(Mutex& mtx, try_to_lock_t) : m(&mtx), owns(mtx.try_lock()) {}
    
    unique_lock(Mutex& mtx, adopt_lock_t) noexcept : m(&mtx), owns(true) {}
    
    ~unique_lock() {
        if (owns)
            m->unlock();
    }
    
    unique_lock(const unique_lock&) = delete;
    unique_lock& operator=(const unique_lock&) = delete;
    
    unique_lock(unique_lock&& u) noexcept : m(u.m), owns(u.owns) {
        u.m = nullptr;
        u.owns = false;
    }
    
    unique_lock& operator=(unique_lock&& u) noexcept {
        if (owns)
            m->unlock();
        m = u.m;
        owns = u.owns;
        u.m = nullptr;
        u.owns = false;
        return *this;
    }
    
    void lock() {
        m->lock();
        owns = true;
    }
    
    bool try_lock() {
        owns = m->try_lock();
        return owns;
    }
    
    void unlock() {
        m->unlock();
        owns = false;
    }
    
    void swap(unique_lock& u) noexcept {
        Mutex* tm = m;
        bool to = owns;
        m = u.m;
        owns = u.owns;
        u.m = tm;
        u.owns = to;
    }
    
    Mutex* release() noexcept {
        Mutex* ret = m;
        m = nullptr;
        owns = false;
        return ret;
    }
    
    bool owns_lock() const noexcept { return owns; }
    explicit operator bool() const noexcept { return owns; }
    Mutex* mutex() const noexcept { return m; }
};

template<typename Mutex>
void swap(unique_lock<Mutex>& x, unique_lock<Mutex>& y) noexcept {
    x.swap(y);
}

struct once_flag {
    atomic<unsigned long> state{0};
    
    once_flag() noexcept = default;
    once_flag(const once_flag&) = delete;
    once_flag& operator=(const once_flag&) = delete;
};

template<typename Callable, typename... Args>
void call_once(once_flag& flag, Callable&& func, Args&&... args) {
    constexpr unsigned long INIT = 0;
    constexpr unsigned long RUNNING = 1;
    constexpr unsigned long DONE = 2;
    
    unsigned long state = flag.state.load(memory_order_acquire);
    
    if (state == DONE)
        return;
    
    if (state == INIT) {
        unsigned long expected = INIT;
        if (flag.state.compare_exchange_strong(expected, RUNNING, 
                                               memory_order_acquire,
                                               memory_order_acquire)) {
            func(static_cast<Args&&>(args)...);
            flag.state.store(DONE, memory_order_release);
            return;
        }
    }
    
    while (flag.state.load(memory_order_acquire) != DONE) {
        #if defined(__x86_64__) || defined(__i386__)
        __builtin_ia32_pause();
        #elif defined(__aarch64__) || defined(__arm__)
        __asm__ __volatile__("yield" ::: "memory");
        #endif
    }
}

} // namespace std